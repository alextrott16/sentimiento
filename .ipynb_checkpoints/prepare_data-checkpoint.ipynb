{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the glove file to use and the save name\n",
    "# (paths are handled automatically, just give the file name)\n",
    "glove_file = 'glove_840B_300d.txt'\n",
    "save_name = 'ready_data_840B_300d.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import cPickle\n",
    "from glover import glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentence/sentiment pairs:\n",
      "Train:\t8117\n",
      "Test:\t2125\n",
      "Dev:\t1044\n",
      "Bad:\t569\n"
     ]
    }
   ],
   "source": [
    "# Get all the data sentences from the data set\n",
    "with open('./data/sentiment_data/datasetSentences.txt', 'r') as f:\n",
    "    S = [line.strip() for line in f]\n",
    "    \n",
    "# Process each sentence into an ID and the sentence\n",
    "sentences = [re.match('\\d+\\\\\\t(.*)', s).group(1) for s in S[1:]]\n",
    "\n",
    "\n",
    "# Get the dictionary of phrases and their IDS\n",
    "with open('./data/sentiment_data/dictionary.txt', 'r') as f:\n",
    "    D = [re.match('([^\\|]*)\\|(\\d+)',line).group(1, 2) for line in f]\n",
    "    \n",
    "D = [(d[0], int(d[1])) for d in D]\n",
    "\n",
    "\n",
    "# Get the sentiment score of each phrase\n",
    "with open('./data/sentiment_data/sentiment_labels.txt', 'r') as f:\n",
    "    tmp = [line.strip() for line in f]\n",
    "\n",
    "sentiments = [re.match('\\d+\\|(.*)',line).group(1) for line in tmp[1:]]\n",
    "sentiments = np.array([float(s) for s in sentiments])\n",
    "\n",
    "\n",
    "# Get the train/dev/test splits\n",
    "with open('./data/sentiment_data/datasetSplit.txt', 'r') as f:\n",
    "    tmp = [line.strip() for line in f]\n",
    "\n",
    "split = [re.match('\\d+\\,(\\d)',line).group(1) for line in tmp[1:]]\n",
    "split = np.array([int(s) for s in split])\n",
    "\n",
    "\n",
    "# Extract the sentiments for each sentence\n",
    "phrase_length   = np.array([len(d[0]) for d in D])\n",
    "sentence_length = np.array([len(s)    for s in sentences])\n",
    "\n",
    "index = np.arange(len(phrase_length))\n",
    "\n",
    "# Sentence sentiments\n",
    "sent_sent = -np.ones(len(sentence_length))\n",
    "\n",
    "# Step through each sentence\n",
    "for sIdx, s in enumerate(sentences):\n",
    "    # Step through the possible matches\n",
    "    match_idx = index[sentence_length[sIdx]==phrase_length]\n",
    "    for i in match_idx:\n",
    "        if s == D[i][0]:\n",
    "            # This is the matching \"phrase\"\n",
    "            # Store its sentiment\n",
    "            sent_sent[sIdx] = sentiments[D[i][1]]\n",
    "            # No need to keep looking\n",
    "            break\n",
    "\n",
    "            \n",
    "# Combine these to form the data set\n",
    "review_target = zip(sentences, sent_sent)\n",
    "\n",
    "# Split this into train/test/dev\n",
    "train_RT = []\n",
    "test_RT  = []\n",
    "dev_RT   = []\n",
    "# Don't assign unscored sentences\n",
    "unscored_RT = []\n",
    "\n",
    "for RT, S in zip(review_target, split):\n",
    "    # If it didn't find a phrase, put it in unscored\n",
    "    if RT[1] == -1.0:\n",
    "        unscored_RT += [RT]\n",
    "    else:\n",
    "        # Use the split list to assign this data pair\n",
    "        if S==1:\n",
    "            train_RT += [RT]\n",
    "        elif S==2:\n",
    "            test_RT += [RT]\n",
    "        else:\n",
    "            dev_RT += [RT]\n",
    "\n",
    "# Print the results\n",
    "print 'Number of sentence/sentiment pairs:\\nTrain:\\t{}\\nTest:\\t{}\\nDev:\\t{}\\nBad:\\t{}'.format(\n",
    "    len(train),len(test),len(dev),len(unscored))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% complete; 332.67 seconds.\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Load the tools for creating sequences of word vectors\n",
    "glove_dir = './data/glove_files/'\n",
    "\n",
    "# Ensure the file ends with .txt\n",
    "if re.search('\\.txt$', glove_file):\n",
    "    glove_file = glove_dir + glove_file\n",
    "else:\n",
    "    glove_file = glove_dir + glove_file + '.txt'\n",
    "\n",
    "G = glove(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function to produce the vector representations of the reviews\n",
    "def make_numeric(G, data):\n",
    "    \"\"\"Assume data are in the [(review, target)] format\"\"\"\n",
    "    \n",
    "    def review2seq(G, review):\n",
    "        \"\"\"For converting a single review\"\"\"\n",
    "        # Tokenize the review\n",
    "        tokens = re.split('\\s+', review)\n",
    "\n",
    "        # Express this token sequence as a vector sequence\n",
    "        return np.concatenate([G.vec(t)[None, :] for t in tokens],\n",
    "                              axis=0)\n",
    "    \n",
    "    \n",
    "    # Pull out the reviews\n",
    "    reviews, targets = zip(*data)\n",
    "    \n",
    "    # Initialize a list of the vector representations\n",
    "    seqs = [None]*len(reviews)\n",
    "    \n",
    "    # Step through each one\n",
    "    for i, r in enumerate(reviews):\n",
    "        # Convert to a sequence of vectors\n",
    "        seqs[i] = review2seq(G, r)\n",
    "        \n",
    "    return zip(seqs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create purely numerical datasets for each usable split\n",
    "train = make_numeric(G, train_RT)\n",
    "test  = make_numeric(G,  test_RT)\n",
    "dev   = make_numeric(G,   dev_RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save it!\n",
    "save_dict = {\n",
    "    'train_RT': train_RT,\n",
    "    'test_RT': test_RT,\n",
    "    'dev_RT': dev_RT,\n",
    "    'unscored_RT': unscored_RT,\n",
    "    'train': train,\n",
    "    'test': test,\n",
    "    'dev': dev}\n",
    "\n",
    "save_dir = './data/'\n",
    "# Ensure the file ends with .pkl\n",
    "if re.search('\\.pkl$', save_name):\n",
    "    save_name = save_dir + save_name\n",
    "else:\n",
    "    save_name = save_dir + save_name + '.pkl'\n",
    "\n",
    "with open(save_name, 'w') as f:\n",
    "    cPickle.dump(save_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
