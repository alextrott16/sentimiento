{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the glove file to use and the save name\n",
    "# (paths are handled automatically, just give the file name)\n",
    "glove_file = 'glove_840B_300d.txt'\n",
    "save_name = 'ready_data_840B_300d.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./sentimiento/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentence/sentiment pairs:\nTrain:\t8117\nTest:\t2125\nDev:\t1044\nBad:\t569\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Get all the data sentences from the data set\n",
    "with open('./data/sentiment_data/datasetSentences.txt', 'r') as f:\n",
    "    S = [line.strip() for line in f]\n",
    "    \n",
    "# Process each sentence into an ID and the sentence\n",
    "sentences = [re.match('\\d+\\\\\\t(.*)', s).group(1) for s in S[1:]]\n",
    "\n",
    "\n",
    "# Get the dictionary of phrases and their IDS\n",
    "with open('./data/sentiment_data/dictionary.txt', 'r') as f:\n",
    "    D = [re.match('([^\\|]*)\\|(\\d+)',line).group(1, 2) for line in f]\n",
    "    \n",
    "D = [(d[0], int(d[1])) for d in D]\n",
    "\n",
    "\n",
    "# Get the sentiment score of each phrase\n",
    "with open('./data/sentiment_data/sentiment_labels.txt', 'r') as f:\n",
    "    tmp = [line.strip() for line in f]\n",
    "\n",
    "sentiments = [re.match('\\d+\\|(.*)',line).group(1) for line in tmp[1:]]\n",
    "sentiments = np.array([float(s) for s in sentiments])\n",
    "\n",
    "\n",
    "# Get the train/dev/test splits\n",
    "with open('./data/sentiment_data/datasetSplit.txt', 'r') as f:\n",
    "    tmp = [line.strip() for line in f]\n",
    "\n",
    "split = [re.match('\\d+\\,(\\d)',line).group(1) for line in tmp[1:]]\n",
    "split = np.array([int(s) for s in split])\n",
    "\n",
    "\n",
    "# Extract the sentiments for each sentence\n",
    "phrase_length   = np.array([len(d[0]) for d in D])\n",
    "sentence_length = np.array([len(s)    for s in sentences])\n",
    "\n",
    "index = np.arange(len(phrase_length))\n",
    "\n",
    "# Sentence sentiments\n",
    "sent_sent = -np.ones(len(sentence_length))\n",
    "\n",
    "# Step through each sentence\n",
    "for sIdx, s in enumerate(sentences):\n",
    "    # Step through the possible matches\n",
    "    match_idx = index[sentence_length[sIdx]==phrase_length]\n",
    "    for i in match_idx:\n",
    "        if s == D[i][0]:\n",
    "            # This is the matching \"phrase\"\n",
    "            # Store its sentiment\n",
    "            sent_sent[sIdx] = sentiments[D[i][1]]\n",
    "            # No need to keep looking\n",
    "            break\n",
    "\n",
    "            \n",
    "# Combine these to form the data set\n",
    "review_target = zip(sentences, sent_sent)\n",
    "\n",
    "# Split this into train/test/dev\n",
    "train_RT = []\n",
    "test_RT  = []\n",
    "dev_RT   = []\n",
    "# Don't assign unscored sentences\n",
    "unscored_RT = []\n",
    "\n",
    "for RT, S in zip(review_target, split):\n",
    "    # If it didn't find a phrase, put it in unscored\n",
    "    if RT[1] == -1.0:\n",
    "        unscored_RT += [RT]\n",
    "    else:\n",
    "        # Use the split list to assign this data pair\n",
    "        if S==1:\n",
    "            train_RT += [RT]\n",
    "        elif S==2:\n",
    "            test_RT += [RT]\n",
    "        else:\n",
    "            dev_RT += [RT]\n",
    "\n",
    "# Print the results\n",
    "print 'Number of sentence/sentiment pairs:\\nTrain:\\t{}\\nTest:\\t{}\\nDev:\\t{}\\nBad:\\t{}'.format(\n",
    "    len(train_RT), len(test_RT), len(dev_RT), len(unscored_RT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing information from ./data/glove_files/glove_840B_300d.txt ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2% complete; about 356.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5% complete; about 357.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7% complete; about 355.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9% complete; about 356.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1% complete; about 357.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4% complete; about 359.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6% complete; about 360.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8% complete; about 361.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0% complete; about 362.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3% complete; about 364.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5% complete; about 364.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7% complete; about 364.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0% complete; about 364.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2% complete; about 364.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4% complete; about 364.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6% complete; about 363.0 seconds left.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9% complete; about 364.0 seconds left.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c674d373fba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mglove_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglove_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mglove_file\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/alex/sentimiento/glover.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, glove_txt_file)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# Store the vector at this index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/re.pyc\u001b[0m in \u001b[0;36msplit\u001b[0;34m(pattern, string, maxsplit, flags)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \"\"\"Split the source string by the occurrences of the pattern,\n\u001b[1;32m    170\u001b[0m     returning a list containing the resulting substrings.\"\"\"\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from glover import glove\n",
    "\n",
    "# Load the tools for creating sequences of word vectors\n",
    "glove_dir = './data/glove_files/'\n",
    "\n",
    "# Ensure the file ends with .txt\n",
    "if re.search('\\.txt$', glove_file):\n",
    "    glove_file = glove_dir + glove_file\n",
    "else:\n",
    "    glove_file = glove_dir + glove_file + '.txt'\n",
    "\n",
    "G = glove(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function to produce the vector representations of the reviews\n",
    "def make_numeric(G, data):\n",
    "    \"\"\"Assume data are in the [(review, target)] format\"\"\"\n",
    "    \n",
    "    def review2seq(G, review):\n",
    "        \"\"\"For converting a single review\"\"\"\n",
    "        # Tokenize the review\n",
    "        tokens = re.split('\\s+', review)\n",
    "\n",
    "        # Express this token sequence as a vector sequence\n",
    "        return np.concatenate([G.vec(t)[None, :] for t in tokens],\n",
    "                              axis=0)\n",
    "    \n",
    "    \n",
    "    # Pull out the reviews\n",
    "    reviews, targets = zip(*data)\n",
    "    \n",
    "    # Initialize a list of the vector representations\n",
    "    seqs = [None]*len(reviews)\n",
    "    \n",
    "    # Step through each one\n",
    "    for i, r in enumerate(reviews):\n",
    "        # Convert to a sequence of vectors\n",
    "        seqs[i] = review2seq(G, r)\n",
    "        \n",
    "    return zip(seqs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create purely numerical datasets for each usable split\n",
    "train = make_numeric(G, train_RT)\n",
    "test  = make_numeric(G,  test_RT)\n",
    "dev   = make_numeric(G,   dev_RT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "\n",
    "# Save it!\n",
    "save_dict = {\n",
    "    'train_RT': train_RT,\n",
    "    'test_RT': test_RT,\n",
    "    'dev_RT': dev_RT,\n",
    "    'unscored_RT': unscored_RT,\n",
    "    'train': train,\n",
    "    'test': test,\n",
    "    'dev': dev}\n",
    "\n",
    "save_dir = './data/'\n",
    "# Ensure the file ends with .pkl\n",
    "if re.search('\\.pkl$', save_name):\n",
    "    save_name = save_dir + save_name\n",
    "else:\n",
    "    save_name = save_dir + save_name + '.pkl'\n",
    "\n",
    "with open(save_name, 'w') as f:\n",
    "    cPickle.dump(save_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}